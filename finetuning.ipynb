{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filter_patents.csv',\n",
       " '.DS_Store',\n",
       " 'test',\n",
       " 'indepn_depn',\n",
       " 'val_sample.csv',\n",
       " 'test_sample.csv',\n",
       " 'filter_patents_.csv',\n",
       " 'train_sample.csv',\n",
       " 'val2006.csv',\n",
       " 'train2006.csv',\n",
       " 'test2006.csv',\n",
       " 'val_sampl_balancede.csv',\n",
       " 'test_sample_balanced.csv',\n",
       " 'Helper doc.docx',\n",
       " 'test_instruction_dataset.csv',\n",
       " 'train',\n",
       " 'train_sample_balanced.csv',\n",
       " 'train_instruction_dataset.csv',\n",
       " 'val_instruction_dataset.csv',\n",
       " 'val']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../cmuMine')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 6)\n",
      "(135, 6)\n",
      "(135, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_dataset= pd.read_csv('train_sample_balanced.csv')\n",
    "test_dataset= pd.read_csv('test_sample_balanced.csv')\n",
    "val_dataset= pd.read_csv('val_sampl_balancede.csv')\n",
    "print(train_dataset.shape)\n",
    "print(test_dataset.shape)\n",
    "print(val_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.10177450989460743\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import warnings\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('test_sample_balanced.csv')\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Ensure EOS token is used for padding if pad token is not defined\n",
    "tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "\n",
    "# Resize model embeddings in case pad token was added\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Create a text generation pipeline\n",
    "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Function to generate text using the model for each abstract and summary\n",
    "def generate_claim(abstract, summary, tokenizer, model, max_length):\n",
    "    # Prepare the prompt with abstract and summary\n",
    "    prompt = \"Given the abstract and summary, generate the first independent claim:\" \\\n",
    "             \"\\nAbstract: \" + abstract + \"\\nSummary: \" + summary\n",
    "    \n",
    "    # Tokenize and truncate the prompt to fit the model's input size\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', max_length=max_length, truncation=True)\n",
    "    \n",
    "    # Generate the output claim\n",
    "    output = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n",
    "    generated_claim = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove the original prompt from the output to get the generated claim only\n",
    "    claim_start = generated_claim.find(\"Summary:\") + len(\"Summary:\")\n",
    "    generated_claim = generated_claim[claim_start:].strip()\n",
    "    return generated_claim\n",
    "\n",
    "# Select the relevant portions of the text and generate the claim\n",
    "test_df['generated_first_claim'] = test_df.apply(\n",
    "    lambda x: generate_claim(x['abstract'], x['summary'], tokenizer, model, max_length=1024),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate the BLEU score\n",
    "references = [[ref.split()] for ref in test_df['firstclaim']]\n",
    "candidates = [cand.split() for cand in test_df['generated_first_claim']]\n",
    "bleu_score = corpus_bleu(references, candidates, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "print(f\"BLEU score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_number</th>\n",
       "      <th>abstract</th>\n",
       "      <th>summary</th>\n",
       "      <th>firstclaim</th>\n",
       "      <th>claims</th>\n",
       "      <th>cpc_class</th>\n",
       "      <th>generated_first_claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US20060247247A1</td>\n",
       "      <td>The present invention provides pyrazole derive...</td>\n",
       "      <td>The present invention pertains to compounds h...</td>\n",
       "      <td>1. A compound having the formula (I), \\n\\n\\n\\n...</td>\n",
       "      <td>1. A compound having the formula (I), \\n\\n\\n\\n...</td>\n",
       "      <td>A</td>\n",
       "      <td>The present invention pertains to compounds ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US20060266357A1</td>\n",
       "      <td>An oxygen concentrator for delivering consiste...</td>\n",
       "      <td>The present invention provides an apparatus t...</td>\n",
       "      <td>1. Apparatus comprising means for producing a ...</td>\n",
       "      <td>1. Apparatus comprising means for producing a ...</td>\n",
       "      <td>A</td>\n",
       "      <td>The present invention provides an apparatus th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US20060204598A1</td>\n",
       "      <td>A method for producing a consumable nicotine-a...</td>\n",
       "      <td>The present invention resides in a method for...</td>\n",
       "      <td>1. A method for producing a consumable nicotin...</td>\n",
       "      <td>1. A method for producing a consumable nicotin...</td>\n",
       "      <td>A</td>\n",
       "      <td>The present invention resides in a method for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US20060261569A1</td>\n",
       "      <td>Mobility assist devices for moving an individu...</td>\n",
       "      <td>Broadly, the present invention provides a plu...</td>\n",
       "      <td>1. A mobility assist device for moving an indi...</td>\n",
       "      <td>1. A mobility assist device for moving an indi...</td>\n",
       "      <td>A</td>\n",
       "      <td>Broadly, the present invention provides a plur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US20060190104A1</td>\n",
       "      <td>An electronic device which may used to enter, ...</td>\n",
       "      <td>An electronic device which may be used to ent...</td>\n",
       "      <td>1. An electronic mountable flip-down apparatus...</td>\n",
       "      <td>1. An electronic mountable flip-down apparatus...</td>\n",
       "      <td>A</td>\n",
       "      <td>An electronic device which may be used to ente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>US20060201003A1</td>\n",
       "      <td>A cutting implement having a pair of cutting b...</td>\n",
       "      <td>It is an object of the present invention to p...</td>\n",
       "      <td>1. A cutting implement comprising: \\na pair of...</td>\n",
       "      <td>1. A cutting implement comprising: \\na pair of...</td>\n",
       "      <td>Y</td>\n",
       "      <td>It is an object of the present invention to pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>US20060201897A1</td>\n",
       "      <td>A shelving system includes an elongated mounti...</td>\n",
       "      <td>According to one aspect of the present invent...</td>\n",
       "      <td>1. A shelving system comprising: \\nan elongate...</td>\n",
       "      <td>1. A shelving system comprising: \\nan elongate...</td>\n",
       "      <td>Y</td>\n",
       "      <td>According to one aspect of the present inventi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>US20060260101A1</td>\n",
       "      <td>The invention relates to a positioning mechani...</td>\n",
       "      <td>The objective of the invention is to propose ...</td>\n",
       "      <td>1. Positioning mechanism for releasably fixing...</td>\n",
       "      <td>1. Positioning mechanism for releasably fixing...</td>\n",
       "      <td>Y</td>\n",
       "      <td>The objective of the invention is to propose a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>US20060251887A1</td>\n",
       "      <td>An antifriction composite system (1) for a bea...</td>\n",
       "      <td>The present invention relates to an antifrict...</td>\n",
       "      <td>1. An antifriction composite multilayer system...</td>\n",
       "      <td>1. An antifriction composite multilayer system...</td>\n",
       "      <td>Y</td>\n",
       "      <td>The present invention relates to an antifricti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>US20060286374A1</td>\n",
       "      <td>An adhesive remaining colored until its use an...</td>\n",
       "      <td>The invention has been accomplished based on ...</td>\n",
       "      <td>1. An adhesive containing microcapsules enclos...</td>\n",
       "      <td>1. An adhesive containing microcapsules enclos...</td>\n",
       "      <td>Y</td>\n",
       "      <td>The invention has been accomplished based on s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       patent_number                                           abstract  \\\n",
       "0    US20060247247A1  The present invention provides pyrazole derive...   \n",
       "1    US20060266357A1  An oxygen concentrator for delivering consiste...   \n",
       "2    US20060204598A1  A method for producing a consumable nicotine-a...   \n",
       "3    US20060261569A1  Mobility assist devices for moving an individu...   \n",
       "4    US20060190104A1  An electronic device which may used to enter, ...   \n",
       "..               ...                                                ...   \n",
       "130  US20060201003A1  A cutting implement having a pair of cutting b...   \n",
       "131  US20060201897A1  A shelving system includes an elongated mounti...   \n",
       "132  US20060260101A1  The invention relates to a positioning mechani...   \n",
       "133  US20060251887A1  An antifriction composite system (1) for a bea...   \n",
       "134  US20060286374A1  An adhesive remaining colored until its use an...   \n",
       "\n",
       "                                               summary  \\\n",
       "0     The present invention pertains to compounds h...   \n",
       "1     The present invention provides an apparatus t...   \n",
       "2     The present invention resides in a method for...   \n",
       "3     Broadly, the present invention provides a plu...   \n",
       "4     An electronic device which may be used to ent...   \n",
       "..                                                 ...   \n",
       "130   It is an object of the present invention to p...   \n",
       "131   According to one aspect of the present invent...   \n",
       "132   The objective of the invention is to propose ...   \n",
       "133   The present invention relates to an antifrict...   \n",
       "134   The invention has been accomplished based on ...   \n",
       "\n",
       "                                            firstclaim  \\\n",
       "0    1. A compound having the formula (I), \\n\\n\\n\\n...   \n",
       "1    1. Apparatus comprising means for producing a ...   \n",
       "2    1. A method for producing a consumable nicotin...   \n",
       "3    1. A mobility assist device for moving an indi...   \n",
       "4    1. An electronic mountable flip-down apparatus...   \n",
       "..                                                 ...   \n",
       "130  1. A cutting implement comprising: \\na pair of...   \n",
       "131  1. A shelving system comprising: \\nan elongate...   \n",
       "132  1. Positioning mechanism for releasably fixing...   \n",
       "133  1. An antifriction composite multilayer system...   \n",
       "134  1. An adhesive containing microcapsules enclos...   \n",
       "\n",
       "                                                claims cpc_class  \\\n",
       "0    1. A compound having the formula (I), \\n\\n\\n\\n...         A   \n",
       "1    1. Apparatus comprising means for producing a ...         A   \n",
       "2    1. A method for producing a consumable nicotin...         A   \n",
       "3    1. A mobility assist device for moving an indi...         A   \n",
       "4    1. An electronic mountable flip-down apparatus...         A   \n",
       "..                                                 ...       ...   \n",
       "130  1. A cutting implement comprising: \\na pair of...         Y   \n",
       "131  1. A shelving system comprising: \\nan elongate...         Y   \n",
       "132  1. Positioning mechanism for releasably fixing...         Y   \n",
       "133  1. An antifriction composite multilayer system...         Y   \n",
       "134  1. An adhesive containing microcapsules enclos...         Y   \n",
       "\n",
       "                                 generated_first_claim  \n",
       "0    The present invention pertains to compounds ha...  \n",
       "1    The present invention provides an apparatus th...  \n",
       "2    The present invention resides in a method for ...  \n",
       "3    Broadly, the present invention provides a plur...  \n",
       "4    An electronic device which may be used to ente...  \n",
       "..                                                 ...  \n",
       "130  It is an object of the present invention to pr...  \n",
       "131  According to one aspect of the present inventi...  \n",
       "132  The objective of the invention is to propose a...  \n",
       "133  The present invention relates to an antifricti...  \n",
       "134  The invention has been accomplished based on s...  \n",
       "\n",
       "[135 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9ebbfa586041cbb8cded6edecac63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3342cedeedc1425595b61631ec8e331e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36695edfbb33481287a83e3f7e04e5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Ensure EOS token is used for padding if pad token is not defined\n",
    "tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "\n",
    "# Resize model embeddings in case pad token was added\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Function to tokenize the input and label pairs for training\n",
    "def tokenize_pairs(examples):\n",
    "    # Tokenization will automatically add the special tokens for GPT-2\n",
    "    model_inputs = tokenizer(examples['input_text'], max_length=1024, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = tokenizer(examples['labels'], max_length=1024, truncation=True, padding='max_length').input_ids\n",
    "    return model_inputs\n",
    "\n",
    "# Prepare the datasets\n",
    "def prepare_dataset(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['input_text'] = \"Abstract: \" + df['abstract'] + \" [SEP] Summary: \" + df['summary']  # Create a prompt\n",
    "    df['labels'] = df['firstclaim'].tolist()  # Expected output\n",
    "    return df\n",
    "\n",
    "# Load and prepare datasets\n",
    "train_df = prepare_dataset('train_sample_balanced.csv')\n",
    "val_df = prepare_dataset('val_sampl_balancede.csv')\n",
    "test_df = prepare_dataset('train_sample_balanced.csv')\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_pairs, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_pairs, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_pairs, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['patent_number', 'abstract', 'summary', 'firstclaim', 'claims', 'cpc_class', 'input_text', 'labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 900\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf52fd06116d46cdbb98b6ffcd7e256f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.6079, 'learning_rate': 1e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7583, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.3094, 'learning_rate': 3e-05, 'epoch': 0.07}\n",
      "{'loss': 1.0335, 'learning_rate': 4e-05, 'epoch': 0.09}\n",
      "{'loss': 1.0382, 'learning_rate': 5e-05, 'epoch': 0.11}\n",
      "{'loss': 1.0042, 'learning_rate': 4.961538461538462e-05, 'epoch': 0.13}\n",
      "{'loss': 1.4264, 'learning_rate': 4.923076923076924e-05, 'epoch': 0.16}\n",
      "{'loss': 1.3512, 'learning_rate': 4.884615384615385e-05, 'epoch': 0.18}\n",
      "{'loss': 1.1543, 'learning_rate': 4.846153846153846e-05, 'epoch': 0.2}\n",
      "{'loss': 1.0932, 'learning_rate': 4.8076923076923084e-05, 'epoch': 0.22}\n",
      "{'loss': 1.0358, 'learning_rate': 4.76923076923077e-05, 'epoch': 0.24}\n",
      "{'loss': 0.849, 'learning_rate': 4.730769230769231e-05, 'epoch': 0.27}\n",
      "{'loss': 1.2313, 'learning_rate': 4.692307692307693e-05, 'epoch': 0.29}\n",
      "{'loss': 0.8991, 'learning_rate': 4.653846153846154e-05, 'epoch': 0.31}\n",
      "{'loss': 0.825, 'learning_rate': 4.615384615384616e-05, 'epoch': 0.33}\n",
      "{'loss': 0.9645, 'learning_rate': 4.576923076923077e-05, 'epoch': 0.36}\n",
      "{'loss': 0.9472, 'learning_rate': 4.538461538461539e-05, 'epoch': 0.38}\n",
      "{'loss': 1.139, 'learning_rate': 4.5e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7905, 'learning_rate': 4.461538461538462e-05, 'epoch': 0.42}\n",
      "{'loss': 0.9662, 'learning_rate': 4.423076923076923e-05, 'epoch': 0.44}\n",
      "{'loss': 0.745, 'learning_rate': 4.384615384615385e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2729, 'learning_rate': 4.346153846153846e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0208, 'learning_rate': 4.3076923076923084e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2674, 'learning_rate': 4.269230769230769e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3316, 'learning_rate': 4.230769230769231e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2485, 'learning_rate': 4.192307692307693e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0339, 'learning_rate': 4.1538461538461544e-05, 'epoch': 0.6}\n",
      "{'loss': 0.9806, 'learning_rate': 4.115384615384615e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9091, 'learning_rate': 4.0769230769230773e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0081, 'learning_rate': 4.038461538461539e-05, 'epoch': 0.67}\n",
      "{'loss': 0.8165, 'learning_rate': 4e-05, 'epoch': 0.69}\n",
      "{'loss': 0.8696, 'learning_rate': 3.961538461538462e-05, 'epoch': 0.71}\n",
      "{'loss': 1.431, 'learning_rate': 3.923076923076923e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9468, 'learning_rate': 3.884615384615385e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0815, 'learning_rate': 3.846153846153846e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0857, 'learning_rate': 3.807692307692308e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0922, 'learning_rate': 3.769230769230769e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3071, 'learning_rate': 3.730769230769231e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0006, 'learning_rate': 3.692307692307693e-05, 'epoch': 0.87}\n",
      "{'loss': 1.0991, 'learning_rate': 3.653846153846154e-05, 'epoch': 0.89}\n",
      "{'loss': 1.0303, 'learning_rate': 3.615384615384615e-05, 'epoch': 0.91}\n",
      "{'loss': 0.9061, 'learning_rate': 3.5769230769230774e-05, 'epoch': 0.93}\n",
      "{'loss': 1.122, 'learning_rate': 3.538461538461539e-05, 'epoch': 0.96}\n",
      "{'loss': 1.0096, 'learning_rate': 3.5e-05, 'epoch': 0.98}\n",
      "{'loss': 0.9545, 'learning_rate': 3.461538461538462e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1447c29f790c42cca733f546e2760c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.867717444896698, 'eval_runtime': 65.4196, 'eval_samples_per_second': 2.064, 'eval_steps_per_second': 0.52, 'epoch': 1.0}\n",
      "{'loss': 0.8974, 'learning_rate': 3.4230769230769234e-05, 'epoch': 1.02}\n",
      "{'loss': 0.7272, 'learning_rate': 3.384615384615385e-05, 'epoch': 1.04}\n",
      "{'loss': 0.919, 'learning_rate': 3.346153846153846e-05, 'epoch': 1.07}\n",
      "{'loss': 0.9717, 'learning_rate': 3.307692307692308e-05, 'epoch': 1.09}\n",
      "{'loss': 1.2155, 'learning_rate': 3.269230769230769e-05, 'epoch': 1.11}\n",
      "{'loss': 0.8276, 'learning_rate': 3.230769230769231e-05, 'epoch': 1.13}\n",
      "{'loss': 0.8378, 'learning_rate': 3.192307692307692e-05, 'epoch': 1.16}\n",
      "{'loss': 1.0223, 'learning_rate': 3.153846153846154e-05, 'epoch': 1.18}\n",
      "{'loss': 0.8595, 'learning_rate': 3.115384615384615e-05, 'epoch': 1.2}\n",
      "{'loss': 0.743, 'learning_rate': 3.0769230769230774e-05, 'epoch': 1.22}\n",
      "{'loss': 1.0427, 'learning_rate': 3.0384615384615382e-05, 'epoch': 1.24}\n",
      "{'loss': 1.0077, 'learning_rate': 3e-05, 'epoch': 1.27}\n",
      "{'loss': 0.9342, 'learning_rate': 2.9615384615384616e-05, 'epoch': 1.29}\n",
      "{'loss': 1.1329, 'learning_rate': 2.9230769230769234e-05, 'epoch': 1.31}\n",
      "{'loss': 1.3051, 'learning_rate': 2.8846153846153845e-05, 'epoch': 1.33}\n",
      "{'loss': 0.9085, 'learning_rate': 2.846153846153846e-05, 'epoch': 1.36}\n",
      "{'loss': 1.2122, 'learning_rate': 2.807692307692308e-05, 'epoch': 1.38}\n",
      "{'loss': 0.878, 'learning_rate': 2.7692307692307694e-05, 'epoch': 1.4}\n",
      "{'loss': 0.9422, 'learning_rate': 2.7307692307692305e-05, 'epoch': 1.42}\n",
      "{'loss': 0.8977, 'learning_rate': 2.6923076923076923e-05, 'epoch': 1.44}\n",
      "{'loss': 0.957, 'learning_rate': 2.6538461538461538e-05, 'epoch': 1.47}\n",
      "{'loss': 0.9137, 'learning_rate': 2.6153846153846157e-05, 'epoch': 1.49}\n",
      "{'loss': 1.0957, 'learning_rate': 2.5769230769230768e-05, 'epoch': 1.51}\n",
      "{'loss': 0.7646, 'learning_rate': 2.5384615384615383e-05, 'epoch': 1.53}\n",
      "{'loss': 0.7241, 'learning_rate': 2.5e-05, 'epoch': 1.56}\n",
      "{'loss': 0.9583, 'learning_rate': 2.461538461538462e-05, 'epoch': 1.58}\n",
      "{'loss': 0.9648, 'learning_rate': 2.423076923076923e-05, 'epoch': 1.6}\n",
      "{'loss': 1.0184, 'learning_rate': 2.384615384615385e-05, 'epoch': 1.62}\n",
      "{'loss': 1.2157, 'learning_rate': 2.3461538461538464e-05, 'epoch': 1.64}\n",
      "{'loss': 1.058, 'learning_rate': 2.307692307692308e-05, 'epoch': 1.67}\n",
      "{'loss': 0.9482, 'learning_rate': 2.2692307692307694e-05, 'epoch': 1.69}\n",
      "{'loss': 0.9782, 'learning_rate': 2.230769230769231e-05, 'epoch': 1.71}\n",
      "{'loss': 0.983, 'learning_rate': 2.1923076923076924e-05, 'epoch': 1.73}\n",
      "{'loss': 1.0388, 'learning_rate': 2.1538461538461542e-05, 'epoch': 1.76}\n",
      "{'loss': 0.9738, 'learning_rate': 2.1153846153846154e-05, 'epoch': 1.78}\n",
      "{'loss': 0.9387, 'learning_rate': 2.0769230769230772e-05, 'epoch': 1.8}\n",
      "{'loss': 0.8467, 'learning_rate': 2.0384615384615387e-05, 'epoch': 1.82}\n",
      "{'loss': 0.8103, 'learning_rate': 2e-05, 'epoch': 1.84}\n",
      "{'loss': 0.9779, 'learning_rate': 1.9615384615384617e-05, 'epoch': 1.87}\n",
      "{'loss': 0.8753, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.89}\n",
      "{'loss': 0.9818, 'learning_rate': 1.8846153846153846e-05, 'epoch': 1.91}\n",
      "{'loss': 0.9103, 'learning_rate': 1.8461538461538465e-05, 'epoch': 1.93}\n",
      "{'loss': 1.1051, 'learning_rate': 1.8076923076923076e-05, 'epoch': 1.96}\n",
      "{'loss': 0.7923, 'learning_rate': 1.7692307692307694e-05, 'epoch': 1.98}\n",
      "{'loss': 0.9318, 'learning_rate': 1.730769230769231e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3073595ad2534887a685445a34d803fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8392157554626465, 'eval_runtime': 57.1042, 'eval_samples_per_second': 2.364, 'eval_steps_per_second': 0.595, 'epoch': 2.0}\n",
      "{'loss': 0.744, 'learning_rate': 1.6923076923076924e-05, 'epoch': 2.02}\n",
      "{'loss': 0.9265, 'learning_rate': 1.653846153846154e-05, 'epoch': 2.04}\n",
      "{'loss': 0.727, 'learning_rate': 1.6153846153846154e-05, 'epoch': 2.07}\n",
      "{'loss': 0.907, 'learning_rate': 1.576923076923077e-05, 'epoch': 2.09}\n",
      "{'loss': 0.9436, 'learning_rate': 1.5384615384615387e-05, 'epoch': 2.11}\n",
      "{'loss': 0.9831, 'learning_rate': 1.5e-05, 'epoch': 2.13}\n",
      "{'loss': 0.9594, 'learning_rate': 1.4615384615384617e-05, 'epoch': 2.16}\n",
      "{'loss': 0.896, 'learning_rate': 1.423076923076923e-05, 'epoch': 2.18}\n",
      "{'loss': 0.8058, 'learning_rate': 1.3846153846153847e-05, 'epoch': 2.2}\n",
      "{'loss': 0.7772, 'learning_rate': 1.3461538461538462e-05, 'epoch': 2.22}\n",
      "{'loss': 1.0485, 'learning_rate': 1.3076923076923078e-05, 'epoch': 2.24}\n",
      "{'loss': 1.1093, 'learning_rate': 1.2692307692307691e-05, 'epoch': 2.27}\n",
      "{'loss': 0.8819, 'learning_rate': 1.230769230769231e-05, 'epoch': 2.29}\n",
      "{'loss': 0.8552, 'learning_rate': 1.1923076923076925e-05, 'epoch': 2.31}\n",
      "{'loss': 1.0423, 'learning_rate': 1.153846153846154e-05, 'epoch': 2.33}\n",
      "{'loss': 0.8577, 'learning_rate': 1.1153846153846154e-05, 'epoch': 2.36}\n",
      "{'loss': 0.7898, 'learning_rate': 1.0769230769230771e-05, 'epoch': 2.38}\n",
      "{'loss': 1.0939, 'learning_rate': 1.0384615384615386e-05, 'epoch': 2.4}\n",
      "{'loss': 0.8049, 'learning_rate': 1e-05, 'epoch': 2.42}\n",
      "{'loss': 0.8155, 'learning_rate': 9.615384615384616e-06, 'epoch': 2.44}\n",
      "{'loss': 1.0771, 'learning_rate': 9.230769230769232e-06, 'epoch': 2.47}\n",
      "{'loss': 0.8049, 'learning_rate': 8.846153846153847e-06, 'epoch': 2.49}\n",
      "{'loss': 0.6547, 'learning_rate': 8.461538461538462e-06, 'epoch': 2.51}\n",
      "{'loss': 0.9562, 'learning_rate': 8.076923076923077e-06, 'epoch': 2.53}\n",
      "{'loss': 0.8444, 'learning_rate': 7.692307692307694e-06, 'epoch': 2.56}\n",
      "{'loss': 0.8271, 'learning_rate': 7.3076923076923085e-06, 'epoch': 2.58}\n",
      "{'loss': 0.8661, 'learning_rate': 6.923076923076923e-06, 'epoch': 2.6}\n",
      "{'loss': 1.092, 'learning_rate': 6.538461538461539e-06, 'epoch': 2.62}\n",
      "{'loss': 0.7421, 'learning_rate': 6.153846153846155e-06, 'epoch': 2.64}\n",
      "{'loss': 0.8266, 'learning_rate': 5.76923076923077e-06, 'epoch': 2.67}\n",
      "{'loss': 1.0392, 'learning_rate': 5.3846153846153855e-06, 'epoch': 2.69}\n",
      "{'loss': 0.7742, 'learning_rate': 5e-06, 'epoch': 2.71}\n",
      "{'loss': 0.8296, 'learning_rate': 4.615384615384616e-06, 'epoch': 2.73}\n",
      "{'loss': 1.1096, 'learning_rate': 4.230769230769231e-06, 'epoch': 2.76}\n",
      "{'loss': 0.9155, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.78}\n",
      "{'loss': 0.9729, 'learning_rate': 3.4615384615384617e-06, 'epoch': 2.8}\n",
      "{'loss': 0.9852, 'learning_rate': 3.0769230769230774e-06, 'epoch': 2.82}\n",
      "{'loss': 0.8669, 'learning_rate': 2.6923076923076928e-06, 'epoch': 2.84}\n",
      "{'loss': 0.7175, 'learning_rate': 2.307692307692308e-06, 'epoch': 2.87}\n",
      "{'loss': 1.1834, 'learning_rate': 1.9230769230769234e-06, 'epoch': 2.89}\n",
      "{'loss': 1.2225, 'learning_rate': 1.5384615384615387e-06, 'epoch': 2.91}\n",
      "{'loss': 0.8523, 'learning_rate': 1.153846153846154e-06, 'epoch': 2.93}\n",
      "{'loss': 0.9242, 'learning_rate': 7.692307692307694e-07, 'epoch': 2.96}\n",
      "{'loss': 1.0337, 'learning_rate': 3.846153846153847e-07, 'epoch': 2.98}\n",
      "{'loss': 0.9254, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da680054128d4c91a82a1626e135ffd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8375254273414612, 'eval_runtime': 292.9751, 'eval_samples_per_second': 0.461, 'eval_steps_per_second': 0.116, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 84443.2577, 'train_samples_per_second': 0.032, 'train_steps_per_second': 0.016, 'train_loss': 1.0744552357991537, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbe65a8502949a6855fb0ffe150bf4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22e5cef35eb4a9480736aaa2e1e0958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/sachin.murthy/Desktop/Patent-Insights/finetuning.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachin.murthy/Desktop/Patent-Insights/finetuning.ipynb#X31sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(bleu_scores) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(bleu_scores)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachin.murthy/Desktop/Patent-Insights/finetuning.ipynb#X31sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Compute BLEU score for the test dataset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sachin.murthy/Desktop/Patent-Insights/finetuning.ipynb#X31sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m avg_bleu_score \u001b[39m=\u001b[39m compute_bleu_score(test_df, model, tokenizer, metric)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachin.murthy/Desktop/Patent-Insights/finetuning.ipynb#X31sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAverage BLEU score on the test set: \u001b[39m\u001b[39m{\u001b[39;00mavg_bleu_score\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/sachin.murthy/Desktop/Patent-Insights/finetuning.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachin.murthy/Desktop/Patent-Insights/finetuning.ipynb#X31sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m bleu_scores \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachin.murthy/Desktop/Patent-Insights/finetuning.ipynb#X31sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(evaluated_dataset)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sachin.murthy/Desktop/Patent-Insights/finetuning.ipynb#X31sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     abstract \u001b[39m=\u001b[39m evaluated_dataset[i][\u001b[39m'\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachin.murthy/Desktop/Patent-Insights/finetuning.ipynb#X31sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     summary \u001b[39m=\u001b[39m evaluated_dataset[i][\u001b[39m'\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachin.murthy/Desktop/Patent-Insights/finetuning.ipynb#X31sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     labels \u001b[39m=\u001b[39m evaluated_dataset[i][\u001b[39m'\u001b[39m\u001b[39mfirstclaim\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    early_stopping_patience=2\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('./finetuned_gpt2_patent')\n",
    "tokenizer.save_pretrained('./finetuned_gpt2_patent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sachin.murthy/Desktop/cmuMine'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filter_patents.csv',\n",
       " '.DS_Store',\n",
       " 'test',\n",
       " 'indepn_depn',\n",
       " 'val_sample.csv',\n",
       " 'test_sample.csv',\n",
       " 'filter_patents_.csv',\n",
       " 'train_sample.csv',\n",
       " 'val2006.csv',\n",
       " 'train2006.csv',\n",
       " 'test2006.csv',\n",
       " 'results',\n",
       " 'val_sampl_balancede.csv',\n",
       " 'test_sample_balanced.csv',\n",
       " 'logs',\n",
       " 'Helper doc.docx',\n",
       " 'test_instruction_dataset.csv',\n",
       " 'finetuned_gpt2_patent',\n",
       " 'train',\n",
       " 'train_sample_balanced.csv',\n",
       " 'train_instruction_dataset.csv',\n",
       " 'val_instruction_dataset.csv',\n",
       " 'val']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.8454613277509645\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "import pandas as pd\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = './finetuned_gpt2_patent'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Load the test dataset\n",
    "test_df= pd.read_csv('test_sample_balanced.csv')  # Replace with the path to your actual test dataset\n",
    "\n",
    "# Ensure EOS token is used for padding if pad token is not defined\n",
    "tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "\n",
    "# Resize model embeddings in case pad token was added\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Create a text generation pipeline\n",
    "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Function to generate text using the model for each abstract and summary\n",
    "def generate_claim(abstract, summary, tokenizer, model, max_length):\n",
    "    # Prepare the prompt with abstract and summary\n",
    "    prompt = \"Given the abstract and summary, generate the firstclaim:\" \\\n",
    "             \"\\nAbstract: \" + abstract + \"\\nSummary: \" + summary\n",
    "    \n",
    "    # Tokenize and truncate the prompt to fit the model's input size\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', max_length=max_length, truncation=True)\n",
    "    \n",
    "    # Generate the output claim\n",
    "    output = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n",
    "    generated_claim = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove the original prompt from the output to get the generated claim only\n",
    "    claim_start = generated_claim.find(\"Summary:\") + len(\"Summary:\")\n",
    "    generated_claim = generated_claim[claim_start:].strip()\n",
    "    return generated_claim\n",
    "\n",
    "# Select the relevant portions of the text and generate the claim\n",
    "test_df['generated_first_claim'] = test_df.apply(\n",
    "    lambda x: generate_claim(x['abstract'], x['summary'], tokenizer, model, max_length=1024),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate the BLEU score\n",
    "references = [[ref.split()] for ref in test_df['firstclaim']]\n",
    "candidates = [cand.split() for cand in test_df['generated_first_claim']]\n",
    "bleu_score = corpus_bleu(references, candidates, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "print(f\"BLEU score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
